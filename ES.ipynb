{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ES.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssinad/evolution-strategies-tensorflow/blob/main/ES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVFcxchkMX_p"
      },
      "source": [
        "# Assignment 1 Part B. Using Evolution Strategies for Non-differentiable Loss Function\n",
        "\n",
        "Notebook adapted from [https://github.com/udacity/CarND-LeNet-Lab/blob/master/LeNet-Lab-Solution.ipynb](https://github.com/udacity/CarND-LeNet-Lab/blob/master/LeNet-Lab-Solution.ipynb)  \n",
        "You can find the paper on evolution strategies at:\n",
        "[https://arxiv.org/abs/1703.03864](https://arxiv.org/abs/1703.03864)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQo_c5hnMX_q"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "First we load the MNIST data, which comes pre-loaded with TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "b2BhhpxVMX_s",
        "outputId": "76e5482d-01dc-4dd8-ef41-65e533c53067"
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False, one_hot=True)\n",
        "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
        "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
        "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
        "\n",
        "assert(len(X_train) == len(y_train))\n",
        "assert(len(X_validation) == len(y_validation))\n",
        "assert(len(X_test) == len(y_test))\n",
        "\n",
        "print()\n",
        "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
        "print()\n",
        "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
        "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
        "print(\"Test Set:       {} samples\".format(len(X_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image Shape: (28, 28, 1)\n",
            "\n",
            "Training Set:   55000 samples\n",
            "Validation Set: 5000 samples\n",
            "Test Set:       10000 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "8DP-3bhQMX_0",
        "outputId": "397fd232-1365-485b-ea86-7a5c1f56105c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Pad images with 0s\n",
        "X_train      = np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "X_validation = np.pad(X_validation, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "X_test       = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "    \n",
        "print(\"Updated Image Shape: {}\".format(X_train[0].shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated Image Shape: (32, 32, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naT3LL3xMX_8"
      },
      "source": [
        "## Setup TensorFlow\n",
        "Here, we define some of the hyperparameters of the network.\n",
        "\n",
        "Also, the training happens really fast with the following hyperparameters. As a result, 2 epochs would suffice to achieve an accuracy of around 90% but I set `EPOCHS` to 15 just to be on the safe side (because of the huge amount of stochasticity in this problem). Feel free to reduce it if you want.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mawetLG7MX_9"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "num_samples = 200\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "acc_sigma = 4.0  # 1.75\n",
        "rate = 5.0 / 55000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgTX6avgMYAB"
      },
      "source": [
        "## SOLUTION: Implement LeNet-5\n",
        "Implement the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
        "\n",
        "### Input\n",
        "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
        "\n",
        "### Architecture\n",
        "**Layer 1: Convolutional.** The output shape should be 28x28x6.\n",
        "\n",
        "**Activation.** Your choice of activation function.\n",
        "\n",
        "**Pooling.** The output shape should be 14x14x6.\n",
        "\n",
        "**Layer 2: Convolutional.** The output shape should be 10x10x16.\n",
        "\n",
        "**Activation.** Your choice of activation function.\n",
        "\n",
        "**Pooling.** The output shape should be 5x5x16.\n",
        "\n",
        "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
        "\n",
        "**Layer 3: Fully Connected.** This should have 120 outputs.\n",
        "\n",
        "**Activation.** Your choice of activation function.\n",
        "\n",
        "**Layer 4: Fully Connected.** This should have 84 outputs.\n",
        "\n",
        "**Activation.** Your choice of activation function.\n",
        "\n",
        "**Layer 5: Fully Connected (Logits).** This should have 10 outputs.\n",
        "\n",
        "### Output\n",
        "Return the result of the logits layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HodYREEhMYAB"
      },
      "source": [
        "from tensorflow.contrib.layers import flatten\n",
        "\n",
        "def LeNet(x):    \n",
        "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
        "    \n",
        "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6\n",
        "    conv1_W = tf.get_variable(\"conv1_W\", shape=(5, 5, 1, 6),\n",
        "           initializer=tf.contrib.layers.xavier_initializer())\n",
        "    conv1_b = tf.Variable(tf.zeros(6))\n",
        "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
        "\n",
        "    # SOLUTION: Activation.\n",
        "    conv1 = tf.nn.relu(conv1)\n",
        "\n",
        "    # SOLUTION: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
        "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x16.\n",
        "    conv2_W = tf.get_variable(\"conv2_W\", shape=(5, 5, 6, 16),\n",
        "           initializer=tf.contrib.layers.xavier_initializer())\n",
        "    conv2_b = tf.Variable(tf.zeros(16))\n",
        "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
        "    \n",
        "    # SOLUTION: Activation.\n",
        "    conv2 = tf.nn.relu(conv2)\n",
        "\n",
        "    # SOLUTION: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
        "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
        "\n",
        "    # SOLUTION: Flatten. Input = 5x5x16. Output = 400.\n",
        "    fc0   = flatten(conv2)\n",
        "    \n",
        "    \n",
        "    # SOLUTION: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
        "#     fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
        "#     fc1_b = tf.Variable(tf.zeros(120))\n",
        "#     fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
        "    fc1 = tf.layers.dense(fc0, 120, name=\"fc1\", \n",
        "                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                         activation=tf.nn.relu)\n",
        "    \n",
        "    # SOLUTION: Activation.\n",
        "#     fc1    = tf.nn.relu(fc1)\n",
        "\n",
        "    # SOLUTION: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
        "#     fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
        "#     fc2_b  = tf.Variable(tf.zeros(84))\n",
        "#     fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
        "\n",
        "    fc2 = tf.layers.dense(fc1, 84, name=\"fc2\", \n",
        "                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                         activation=tf.nn.relu)\n",
        "    \n",
        "    # SOLUTION: Activation.\n",
        "#     fc2    = tf.nn.relu(fc2)\n",
        "\n",
        "    logits = tf.layers.dense(fc2, 10, name=\"logits\", \n",
        "                         kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "    \n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijFKIjQbMYAE"
      },
      "source": [
        "## Features and Labels\n",
        "Train LeNet to classify [MNIST](http://yann.lecun.com/exdb/mnist/) data.\n",
        "\n",
        "`x` is a placeholder for a batch of input images.\n",
        "`y` is a placeholder for a batch of output labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBIAdBmNMYAF"
      },
      "source": [
        "x = tf.placeholder(tf.float32, (None, 32, 32, 1), name=\"input_data\")\n",
        "one_hot_y = tf.placeholder(tf.int32, (None, 10), name='true_labels')\n",
        "# epsi = tf.placeholder(tf.float32, shape=(None, 10))  \n",
        "# one_hot_y = tf.one_hot(y, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfz5Ls2qMYAI"
      },
      "source": [
        "## Training Pipeline\n",
        "Create a training pipeline that uses the model to classify MNIST data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-rn35mTMYAJ"
      },
      "source": [
        "logits = LeNet(x)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLV2RF3ipt52"
      },
      "source": [
        "## Estimating the gradient using ES algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueYjEzB5pt59"
      },
      "source": [
        "At first, I used `tf.while_loop` for gradient estimation but eventually I found a shorter (and hopefully more efficient) way to do it.\n",
        "$$F=1-Accuracy$$\n",
        "$$ \\nabla F(l) \\approx \\frac{1}{n\\sigma}\\sum_{i=1}^{n}{F(l + \\sigma\\epsilon_i)\\epsilon_i} $$\n",
        "where $n$ is `num_samples`, $\\sigma$ is `acc_sigma` and $l$ is `logits`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwDHnpx4MYAN"
      },
      "source": [
        "# cnt = tf.constant(0)\n",
        "# acc_mean = tf.Variable(tf.zeros([BATCH_SIZE, 10]), name=\"acc_mean\", trainable=False, validate_shape=False)\n",
        "# indices = tf.where(tf.equal(tf.argmax(logits + acc_sigma * epsi, 1), tf.argmax(one_hot_y, 1)))\n",
        "# acc_mean = tf.reduce_sum(1-tf.gather_nd(epsi, indices=indices) / acc_sigma / num_samples, axis=0)\n",
        "\n",
        "log_repl = tf.expand_dims(logits, 2)\n",
        "y_repl = tf.expand_dims(one_hot_y, 2)\n",
        "# log_repl = tf.reshape(tf.tile(logits, [num_samples]), [1, 1, num_samples)\n",
        "# y_repl = tf.reshape(tf.tile(one_hot_y, num_samples), [1, 1, num_samples])\n",
        "epsi = tf.random_normal([tf.shape(one_hot_y)[0], tf.shape(one_hot_y)[1], num_samples])\n",
        "\n",
        "F = tf.cast(tf.not_equal(tf.argmax(log_repl + acc_sigma * epsi, 1), tf.argmax(y_repl, 1)), tf.float32)\n",
        "acc_mean = tf.reduce_mean(tf.expand_dims(F, 1) * epsi, axis=2) / acc_sigma\n",
        "\n",
        "# # acc_sum2 = tf.Variable(0.0, name=\"acc_sum2\", trainable=False)\n",
        "# acc_sum2 = tf.zeros(10)\n",
        "# acc_mean2 = tf.zeros(10)\n",
        "# def cond(i, assign):\n",
        "#     return i < tf.shape(x)[0]\n",
        "#     return True\n",
        "# def body(i, assign):\n",
        "#   global acc_mean\n",
        "#   epsi = tf.random_normal(tf.shape(logits))\n",
        "#   indices = tf.where(tf.not_equal(tf.argmax(logits[i, :] + acc_sigma * epsi, 1), tf.argmax(one_hot_y[i, :], 1)))\n",
        "#   assign = tf.assign(acc_mean[i, :], tf.reduce_sum(tf.gather_nd(epsi, indices=indices) / acc_sigma / num_samples, axis=0), validate_shape=False)\n",
        "#   i += 1\n",
        "#   return i, assign\n",
        "# # #     global acc_sum2, acc_mean2\n",
        "#     F = -tf.cast(tf.equal(my_argmax[i],\n",
        "#                                     tf.argmax(one_hot_y, 1)), tf.float32)\n",
        "#     sum_so_far += F * epsi[i, :] / acc_sigma / num_samples\n",
        "#     sum_so_far.set_shape([10])\n",
        "# #     # acc_mean2 += (F * epsi / acc_sigma - acc_mean2) / (i + 1)\n",
        "#     i += 1\n",
        "# #     i += tf.shape(F)[0]\n",
        "# #     sum_so_far /= tf.cast(i, tf.float32)\n",
        "#     return i, sum_so_far\n",
        "# cnt, acc_mean = tf.while_loop(cond, body, [tf.constant(0), tf.zeros([10])])\n",
        "# cnt, assign =  tf.while_loop(cond, body, [tf.constant(0), tf.constant(0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0aeLw3fpt6D"
      },
      "source": [
        "## Using the Estimate in the Training Pipeline\n",
        "### First Attempt : RegisterGradient\n",
        "My first attempt was to define gradients for the non-differentiable operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT_PlJoTpt6E"
      },
      "source": [
        "# Define new operation and gradient\n",
        "# tf.Graph.create_op(\"NegAcc\")\n",
        "# https://uoguelph-mlrg.github.io/tensorflow_gradients/\n",
        "\n",
        "# @tf.RegisterGradient(\"NegAccGrad\")\n",
        "# def neg_acc_grad(op, grad):\n",
        "    \n",
        "#     print(tmp.shape)\n",
        "#     print(op.inputs[0].shape, tf.constant(1.0).shape)\n",
        "#     return tf.expand_dims(tf.constant(1.0), 0), None  # tf.constant(0.0)\n",
        "\n",
        "# @tf.RegisterGradient(\"AccSumGrad\")\n",
        "# def acc_sum_grad(op, grad):\n",
        "#   return acc_sum\n",
        "# @tf.RegisterGradient(\"ArgMaxGrad\")\n",
        "# def arg_max_grad(op, grad):\n",
        "#     tmp = acc_mean\n",
        "#     print(tmp.shape)\n",
        "#     return tmp, None\n",
        "\n",
        "# with tf.get_default_graph().gradient_override_map({'Equal': 'NegAccGrad',\n",
        "#                                                    'Cast': 'Identity', \"ArgMax\": \"ArgMaxGrad\"}):\n",
        "#     loss_operation = tf.cast(tf.equal(tf.argmax(logits + acc_sigma * epsi, 1),\n",
        "#                                        tf.argmax(one_hot_y, 1)), tf.float32)\n",
        "# print(loss_operation.shape)\n",
        "# with tf.get_default_graph().gradient_override_map({'acc_sum': 'AccSumGrad'}):\n",
        "#   loss_operation = acc_mean + tf.stop_gradient(tf.cast(tf.equal(tf.argmax(logits, 1),\n",
        "#                                        tf.argmax(one_hot_y, 1)), tf.float32) - acc_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT3JKNIPpt6P"
      },
      "source": [
        "### Second Attempt: Using Taylor Series to Approximate F\n",
        "I also tried using the following loss (inspired by taylor series) to approximate the non-differentiable loss function. This was a successful attempt and has been used in this notebook.\n",
        "$$Loss = \\nabla F(l) l \\Longrightarrow \\nabla_l Loss = \\nabla F(l)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDh2vIx5pt6R"
      },
      "source": [
        "# F = -tf.cast(tf.equal(tf.argmax(logits, 1),\n",
        "#                                     tf.argmax(one_hot_y, 1)), tf.float32)\n",
        "# loss_operation = tf.stop_gradient(F) + tf.matmul(logits, tf.expand_dims(tf.stop_gradient(acc_mean), 1), name=\"taylor_series\")\n",
        "# loss_operation = tf.matmul(logits, tf.stop_gradient(tf.expand_dims(acc_mean, 1)), name=\"taylor_series\")\n",
        "prod = tf.multiply(tf.stop_gradient(acc_mean), logits)\n",
        "loss_operation = tf.reduce_sum(prod, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySfjyI8Apt6Z"
      },
      "source": [
        "### Third Attempt: Gradients Multiplication\n",
        "\n",
        "Another one of my attempts was to compute the gradients of the logits and then multiply the very last one by the estimated gradient.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGn5Ndoypt6b"
      },
      "source": [
        "# training_operation = optimizer.minimize(loss_operation, global_step=global_step)\n",
        "# grads_and_vars = optimizer.compute_gradients(logits, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
        "\n",
        "# grads = [grad for grad in tf.gradients(logits, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)) if grad is not None]\n",
        "# last_grad = tf.get_default_graph().get_tensor_by_name(name=\"gradients_2/add_4_grad/Reshape_1:0\")\n",
        "# last_grad = grads[-1]\n",
        "# last_grad = acc_mean + last_grad\n",
        "\n",
        "# last_grad = grads[-2]\n",
        "# last_grad = tf.get_default_graph().get_tensor_by_name(name=\"gradients_2/MatMul_2_grad/MatMul_1:0\")\n",
        "# last_grad = tf.matmul(last_grad, tf.expand_dims(acc_mean, 1))\n",
        "\n",
        "# for grad, var in grads_and_vars:\n",
        "#   if grad is not None:\n",
        "#     grad *= tf.gacc_mean\n",
        "# training_operation = optimizer.apply_gradients(grads_and_vars)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkWVK9dPMYAT"
      },
      "source": [
        "### Last Attempt: Using the Gradient Estimate to Update Logits\n",
        "\n",
        "This attempt was in accordance with Prof. Ray's proposition and successful. \n",
        "$$logits_{new} = logits - \\frac{\\alpha\n",
        "}{n\\sigma}\\sum_{i=1}^{n}{F(logits + \\sigma\\epsilon_i)\\epsilon_i} $$\n",
        "$$Loss = \\big|\\big|logits_{new} - logits\\big|\\big|_2^2$$\n",
        "However, I chose the second one over this, specifically because here we are doing the logit update manually whereas in the second approach, we come up with a loss function and let the Optimizer (in this case Adam) handle the update."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUM0T69UMYAV"
      },
      "source": [
        "# new_logits = logits - rate * acc_mean\n",
        "# loss_operation = tf.losses.mean_squared_error(labels=tf.stop_gradient(new_logits), predictions=logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvKZ63t4DrdY"
      },
      "source": [
        "## Computing the Gradients of the Network Parameters\n",
        "\n",
        "After defining a loss function for this problem, we compute its gradient with respect to the trainable network parameters and use it to learn them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZbMaKiRDniS"
      },
      "source": [
        "grads_and_vars = optimizer.compute_gradients(loss_operation, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
        "training_operation = optimizer.apply_gradients(grads_and_vars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLNAgP6OWAuS"
      },
      "source": [
        "## Creating Summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRGqD1iiV8mO"
      },
      "source": [
        "def add_gradient_summaries(grads_and_vars):\n",
        "    for grad, var in grads_and_vars:\n",
        "        if grad is not None:\n",
        "            tf.summary.histogram(var.op.name + \"/gradient\", grad)\n",
        "\n",
        "for var in tf.trainable_variables():\n",
        "    tf.summary.histogram(var.op.name + \"/histogram\", var)\n",
        "\n",
        "add_gradient_summaries(grads_and_vars)\n",
        "merged_summary_op= tf.summary.merge_all()\n",
        "summary_writer = tf.summary.FileWriter('logs-student/')\n",
        "summary_writer.add_graph(tf.get_default_graph())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLCMuWprMYAY"
      },
      "source": [
        "## Model Evaluation\n",
        "The function `evaluate` evaluates the accuracy of the model for a given dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri5xoWqaMYAZ"
      },
      "source": [
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
        "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "def evaluate(X_data, y_data):\n",
        "    eval_batch_size = 128\n",
        "    num_examples = len(X_data)\n",
        "    total_accuracy = 0\n",
        "    sess = tf.get_default_session()\n",
        "    for offset in range(0, num_examples, eval_batch_size):\n",
        "        batch_x, batch_y = X_data[offset:offset+eval_batch_size], y_data[offset:offset+eval_batch_size]\n",
        "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, one_hot_y: batch_y})\n",
        "        total_accuracy += (accuracy * len(batch_x))\n",
        "    return total_accuracy / num_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvFYzRh1MYAb"
      },
      "source": [
        "## Train the Model\n",
        "Here, we run the training data through the training pipeline to train the model.\n",
        "\n",
        "The model is saved after training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3468
        },
        "id": "g9yR_AeoMYAe",
        "outputId": "20eb65ef-e115-4d60-ef74-9fcce8b8ca75"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    num_examples = len(X_train)\n",
        "    \n",
        "    print(\"Training...\")\n",
        "    print()\n",
        "    for i in range(EPOCHS):\n",
        "        X_train, y_train = shuffle(X_train, y_train)\n",
        "        for offset in range(0, num_examples, BATCH_SIZE):\n",
        "            end = offset + BATCH_SIZE\n",
        "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
        "\n",
        "            _, summaries = sess.run([training_operation, merged_summary_op],\n",
        "                                      feed_dict={x: batch_x, one_hot_y: batch_y})\n",
        "\n",
        "#             if end % 5000 == 0: \n",
        "#                 summary_writer.add_summary(summaries, global_step=offset)\n",
        "#                 validation_accuracy = evaluate(X_validation, y_validation)\n",
        "#                 print(\"{} of 55000\\nValidation Accuracy = {:.3f}\".format(end, validation_accuracy))\n",
        "\n",
        "        print()\n",
        "        print(\"EPOCH {} ...\".format(i+1))\n",
        "        validation_accuracy = evaluate(X_validation, y_validation)\n",
        "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
        "        print()\n",
        "        \n",
        "    saver.save(sess, './lenet')\n",
        "    print(\"Model saved\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\n",
            "\n",
            "EPOCH 1 ...\n",
            "Validation Accuracy = 0.787\n",
            "\n",
            "\n",
            "EPOCH 2 ...\n",
            "Validation Accuracy = 0.831\n",
            "\n",
            "\n",
            "EPOCH 3 ...\n",
            "Validation Accuracy = 0.841\n",
            "\n",
            "\n",
            "EPOCH 4 ...\n",
            "Validation Accuracy = 0.852\n",
            "\n",
            "\n",
            "EPOCH 5 ...\n",
            "Validation Accuracy = 0.855\n",
            "\n",
            "\n",
            "EPOCH 6 ...\n",
            "Validation Accuracy = 0.858\n",
            "\n",
            "\n",
            "EPOCH 7 ...\n",
            "Validation Accuracy = 0.862\n",
            "\n",
            "\n",
            "EPOCH 8 ...\n",
            "Validation Accuracy = 0.862\n",
            "\n",
            "\n",
            "EPOCH 9 ...\n",
            "Validation Accuracy = 0.867\n",
            "\n",
            "\n",
            "EPOCH 10 ...\n",
            "Validation Accuracy = 0.870\n",
            "\n",
            "\n",
            "EPOCH 11 ...\n",
            "Validation Accuracy = 0.872\n",
            "\n",
            "\n",
            "EPOCH 12 ...\n",
            "Validation Accuracy = 0.874\n",
            "\n",
            "\n",
            "EPOCH 13 ...\n",
            "Validation Accuracy = 0.872\n",
            "\n",
            "\n",
            "EPOCH 14 ...\n",
            "Validation Accuracy = 0.878\n",
            "\n",
            "\n",
            "EPOCH 15 ...\n",
            "Validation Accuracy = 0.877\n",
            "\n",
            "\n",
            "EPOCH 16 ...\n",
            "Validation Accuracy = 0.879\n",
            "\n",
            "\n",
            "EPOCH 17 ...\n",
            "Validation Accuracy = 0.877\n",
            "\n",
            "\n",
            "EPOCH 18 ...\n",
            "Validation Accuracy = 0.881\n",
            "\n",
            "\n",
            "EPOCH 19 ...\n",
            "Validation Accuracy = 0.879\n",
            "\n",
            "\n",
            "EPOCH 20 ...\n",
            "Validation Accuracy = 0.884\n",
            "\n",
            "\n",
            "EPOCH 21 ...\n",
            "Validation Accuracy = 0.883\n",
            "\n",
            "\n",
            "EPOCH 22 ...\n",
            "Validation Accuracy = 0.881\n",
            "\n",
            "\n",
            "EPOCH 23 ...\n",
            "Validation Accuracy = 0.956\n",
            "\n",
            "\n",
            "EPOCH 24 ...\n",
            "Validation Accuracy = 0.960\n",
            "\n",
            "\n",
            "EPOCH 25 ...\n",
            "Validation Accuracy = 0.962\n",
            "\n",
            "\n",
            "EPOCH 26 ...\n",
            "Validation Accuracy = 0.965\n",
            "\n",
            "\n",
            "EPOCH 27 ...\n",
            "Validation Accuracy = 0.967\n",
            "\n",
            "\n",
            "EPOCH 28 ...\n",
            "Validation Accuracy = 0.971\n",
            "\n",
            "\n",
            "EPOCH 29 ...\n",
            "Validation Accuracy = 0.970\n",
            "\n",
            "\n",
            "EPOCH 30 ...\n",
            "Validation Accuracy = 0.972\n",
            "\n",
            "\n",
            "EPOCH 31 ...\n",
            "Validation Accuracy = 0.975\n",
            "\n",
            "\n",
            "EPOCH 32 ...\n",
            "Validation Accuracy = 0.974\n",
            "\n",
            "\n",
            "EPOCH 33 ...\n",
            "Validation Accuracy = 0.974\n",
            "\n",
            "\n",
            "EPOCH 34 ...\n",
            "Validation Accuracy = 0.976\n",
            "\n",
            "\n",
            "EPOCH 35 ...\n",
            "Validation Accuracy = 0.975\n",
            "\n",
            "\n",
            "EPOCH 36 ...\n",
            "Validation Accuracy = 0.975\n",
            "\n",
            "\n",
            "EPOCH 37 ...\n",
            "Validation Accuracy = 0.974\n",
            "\n",
            "\n",
            "EPOCH 38 ...\n",
            "Validation Accuracy = 0.977\n",
            "\n",
            "\n",
            "EPOCH 39 ...\n",
            "Validation Accuracy = 0.978\n",
            "\n",
            "\n",
            "EPOCH 40 ...\n",
            "Validation Accuracy = 0.980\n",
            "\n",
            "\n",
            "EPOCH 41 ...\n",
            "Validation Accuracy = 0.978\n",
            "\n",
            "\n",
            "EPOCH 42 ...\n",
            "Validation Accuracy = 0.978\n",
            "\n",
            "\n",
            "EPOCH 43 ...\n",
            "Validation Accuracy = 0.978\n",
            "\n",
            "\n",
            "EPOCH 44 ...\n",
            "Validation Accuracy = 0.979\n",
            "\n",
            "\n",
            "EPOCH 45 ...\n",
            "Validation Accuracy = 0.980\n",
            "\n",
            "\n",
            "EPOCH 46 ...\n",
            "Validation Accuracy = 0.979\n",
            "\n",
            "\n",
            "EPOCH 47 ...\n",
            "Validation Accuracy = 0.979\n",
            "\n",
            "\n",
            "EPOCH 48 ...\n",
            "Validation Accuracy = 0.981\n",
            "\n",
            "\n",
            "EPOCH 49 ...\n",
            "Validation Accuracy = 0.980\n",
            "\n",
            "\n",
            "EPOCH 50 ...\n",
            "Validation Accuracy = 0.980\n",
            "\n",
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfHq7J3QMYAh"
      },
      "source": [
        "## Evaluate the Model\n",
        "Here, we evaluate the performance of the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "JXPLTK8iMYAh",
        "outputId": "a267d2ad-cb4b-4daa-883e-bd5811f681ee"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "    \n",
        "    test_accuracy = evaluate(X_test, y_test)\n",
        "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./lenet\n",
            "Test Accuracy = 0.978\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}